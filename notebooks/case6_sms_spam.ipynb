{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL3byIHfWEVx"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOTWElUCWk_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bceaa9d-fc05-4ba0-9003-86371853c49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# For GPU check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x953lw83WxnY"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define model and tokenizer parameters\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "max_seq_length = 1024           # Maximum sequence length for training/inference\n",
        "dtype = None                     # Let the library auto-detect optimal data type\n",
        "\n",
        "# Load the pre-trained LLaMA model and tokenizer with 4-bit quantization\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIdADxFWXToO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the UC Irvine SMS Spam dataset\n",
        "dataset = load_dataset(\"ucirvine/sms_spam\")\n",
        "\n",
        "# Inspect the column names to verify structure\n",
        "print(dataset[\"train\"].column_names)  # Expecting 'sms' for message, 'label' for class\n",
        "\n",
        "# Format each example into the required prompt format for fine-tuning\n",
        "def format_prompt(example):\n",
        "    label_str = \"HAM\" if example['label'] == 0 else \"SPAM\"\n",
        "    return f\"### Text: {example['sms']}\\n### Label: {label_str}<|endoftext|>\"\n",
        "\n",
        "# Apply formatting to all dataset splits and remove original columns\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"text\": format_prompt(x)},\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original training dataset into training and validation sets\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Assign splits to separate variables for clarity\n",
        "train_dataset = train_test[\"train\"]   # 80% for training\n",
        "val_dataset = train_test[\"test\"]      # 20% for validation\n"
      ],
      "metadata": {
        "id": "eLtAcTuGqZ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08de3wAXdu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78b798d-2f07-4212-a921-9bd3db6f5290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.11.4 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA adapters to the base model for parameter-efficient fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # LoRA rank - controls adapter capacity\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Layers to apply LoRA\n",
        "    lora_alpha=64,  # Scaling factor for LoRA updates\n",
        "    lora_dropout=0.05,  # Dropout applied within LoRA layers for regularization\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm8booC8XliQ"
      },
      "outputs": [],
      "source": [
        "# Initialize SFTTrainer for supervised fine-tuning with LoRA-adapted model\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,  # Dataset used for training the model\n",
        "    eval_dataset=val_dataset,     # Dataset used for evaluation during training\n",
        "    dataset_text_field=\"text\",    # Field in the dataset containing the text input\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # Number of samples per GPU per step\n",
        "        gradient_accumulation_steps=4,  # Accumulate gradients for effective batch size\n",
        "        warmup_steps=20,                # Linear warmup steps for learning rate scheduler\n",
        "        num_train_epochs=3,             # Total number of training epochs\n",
        "        learning_rate=2e-4,             # Learning rate for optimizer\n",
        "        fp16=True,                      # Mixed-precision training for faster performance\n",
        "        logging_steps=25,               # Log metrics every N steps\n",
        "        optim=\"adamw_8bit\",             # Optimizer with 8-bit precision for memory efficiency\n",
        "        weight_decay=0.1,               # Weight decay for regularization\n",
        "        lr_scheduler_type=\"linear\",     # Linear scheduler for learning rate\n",
        "        seed=3407,                      # Seed for reproducibility\n",
        "        output_dir=\"outputs\",           # Directory to save model checkpoints\n",
        "        report_to=\"none\",               # Disable automatic logging (e.g., WandB)\n",
        "        save_strategy=\"no\"              # No automatic saving of checkpoints during training\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZrtr0c4XmTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "578ba69d-12bc-4767-8b2c-ed781adb73a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 4,459 | Num Epochs = 3 | Total steps = 1,674\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 18,350,080 of 3,231,099,904 (0.57% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1674' max='1674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1674/1674 58:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.703900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.892800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.712500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.526600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.493000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.480100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.349400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.347900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>2.397900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>2.424300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.372100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>2.404500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>2.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.329400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>2.350800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.232800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>2.292500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>2.077900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.043400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>1.965700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>1.971900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>1.965900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>1.933900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.994100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>1.987500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.100400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>1.903900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.019200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>1.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.988200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>1.820300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>1.843300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.914700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>1.981300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.892200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>1.872700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.628700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>1.649800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>1.572700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>1.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.601500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>1.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>1.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>1.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>1.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.575100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>1.688600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>1.594800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>1.645600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>1.469200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.522900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vBGnx7DXuN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323c1fad-19e1-441a-b01a-3bb32cad8e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "Accuracy: 0.931, Precision: 0.686, Recall: 0.893, F1: 0.776\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Prepare model for inference mode to optimize generation\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Initialize lists to store evaluation results\n",
        "TP, TN, FP, FN = [], [], [], []\n",
        "all_results = []\n",
        "\n",
        "# Iterate over validation dataset to generate predictions\n",
        "for idx, row in val_dataset.to_pandas().iterrows():\n",
        "    text_input = row['text']\n",
        "    # Determine ground-truth label based on dataset content\n",
        "    true_label = \"HAM\" if \"HAM\" in text_input else \"SPAM\"\n",
        "\n",
        "    # Apply chat template and tokenize input for model\n",
        "    messages = [{\"role\": \"user\", \"content\": text_input}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate text predictions using the model\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Decode only the generated portion of the output\n",
        "    pred_output = tokenizer.batch_decode(outputs[:, inputs.shape[1]:])[0].strip().upper()\n",
        "\n",
        "    # Parse predicted label from generated text\n",
        "    pred_label = \"SPAM\" if \"SPAM\" in pred_output else \"HAM\"\n",
        "\n",
        "    # Categorize prediction as TP, TN, FP, or FN\n",
        "    match = \"\"\n",
        "    if true_label == \"SPAM\":\n",
        "        if pred_label == \"SPAM\":\n",
        "            TP.append(text_input)\n",
        "            match = \"TP\"\n",
        "        else:\n",
        "            FN.append(text_input)\n",
        "            match = \"FN\"\n",
        "    else:\n",
        "        if pred_label == \"SPAM\":\n",
        "            FP.append(text_input)\n",
        "            match = \"FP\"\n",
        "        else:\n",
        "            TN.append(text_input)\n",
        "            match = \"TN\"\n",
        "\n",
        "    # Store detailed result for analysis\n",
        "    all_results.append([text_input, true_label, pred_label, match])\n",
        "\n",
        "# Compute evaluation metrics\n",
        "num_TP, num_TN, num_FP, num_FN = len(TP), len(TN), len(FP), len(FN)\n",
        "accuracy = (num_TP + num_TN) / (num_TP + num_TN + num_FP + num_FN)\n",
        "precision = num_TP / (num_TP + num_FP) if (num_TP + num_FP) > 0 else 0\n",
        "recall = num_TP / (num_TP + num_FN) if (num_TP + num_FN) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Print metrics summary\n",
        "print(\"Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "# Convert evaluation results into a structured DataFrame for exporting\n",
        "results_df = pd.DataFrame(\n",
        "    all_results,\n",
        "    columns=[\n",
        "        \"text\",         # Original input text\n",
        "        \"true_label\",   # Ground-truth label (HAM / SPAM)\n",
        "        \"pred_label\",   # Model predicted label (HAM / SPAM)\n",
        "        \"result_type\"   # Classification outcome: TP, TN, FP, FN\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save results to Excel for further analysis or reporting\n",
        "results_df.to_excel(\"model_predictions_comparison.xlsx\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twEmkIrLZLtD"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmERd43la0l"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Locate the fine-tuned GGUF model file and download it to the local machine for use or sharing\n",
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "    print(f\"Downloading: {gguf_file}\")\n",
        "    files.download(gguf_file)\n"
      ]
    }
  ]
}